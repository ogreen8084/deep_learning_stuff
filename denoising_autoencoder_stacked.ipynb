{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1>Stacked Denoising Autencoder</h1>\n",
    "\n",
    "This is an implemenation of a stacked denoising autoencoder to classify MNIST digits. This will be a greedy implemenation in which the each of the encoding layers will be trained to reproduce its input and noise will be injected using Additive Gaussian Noise at each layer. Each layer will be fed the uncorrupted version of the prior layer. Likewise when the softmax layer is added to the network to classify digits it will be fed the uncorrupted version.\n",
    "\n",
    "The encoding layers and the classifier are independently trained. The second encoding layer is fed a trained version of the output of the first encoding layer. The classifier is fed a trained version of the output of the second encoding layer. An advantage of this network is that larger networks are more complex to train, but obviously since its a greedy implmentation there's no guarantee that there is an optimal coordination of the weights between network\n",
    "layers. \n",
    "\n",
    "Finally, we will compare our encoded dataset's performance (50 dimensions) to the performance on the original dataset (784 dimensions). Note that the batch function is random so it is not a true one to one comparision, but nevertheless it should be a pretty accurate way of comparing performance. \n",
    "\n",
    "I learned about the network here:\n",
    "http://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf\n",
    "\n",
    "Loosely followed:\n",
    "https://github.com/tensorflow/models/blob/master/autoencoder/autoencoder_models/DenoisingAutoencoder.py (simple implementaiton of adding the noise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Dependencies:\n",
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2>Architecture variables</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_dim = 784\n",
    "batch_size = 128\n",
    "plots_outdir=\"./png\"\n",
    "n_hidden1 = 500\n",
    "n_hidden2 = 50\n",
    "scale = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2>Encoding Layer 1 Training</h2>\n",
    "\n",
    "We train the first encoding layer by adding gaussian noise, then try to recreate the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    x_in = tf.placeholder(tf.float32, (None, input_dim))\n",
    "    drop = tf.placeholder(tf.float32, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with train_graph.as_default():\n",
    "    with tf.variable_scope('enc_one'):\n",
    "        enc_w1 = tf.get_variable(\"enc_1\", shape=[input_dim, n_hidden1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        enc_b1 = tf.get_variable('b_1', shape=(n_hidden1), initializer = tf.constant_initializer(0.1))\n",
    "        dec_greed_w1 = tf.get_variable('dec_greed_1', shape=[n_hidden1, input_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        dec_greed_b1 = tf.get_variable('dec_greed_b1', shape=(input_dim), initializer=tf.constant_initializer(0.1))             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with train_graph.as_default():\n",
    "    enc_1_hidden= tf.nn.relu(tf.add(tf.matmul(x_in + scale * tf.random_normal((input_dim,)), enc_w1), enc_b1))\n",
    "    dec_1_output = tf.nn.sigmoid(tf.add(tf.matmul(enc_1_hidden, dec_greed_w1), dec_greed_b1))\n",
    "    cost1 = 0.5 * tf.reduce_sum(tf.pow(tf.subtract(x_in , dec_1_output), 2.0))\n",
    "    with tf.variable_scope('optim'):\n",
    "        optimizer1 = tf.train.AdamOptimizer( name='optim1').minimize(cost1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 cost: 12835.944336\n",
      "Iteration: 200 cost: 1008.949646\n",
      "Iteration: 400 cost: 552.172302\n",
      "Iteration: 600 cost: 344.026093\n",
      "Iteration: 800 cost: 334.097839\n",
      "Iteration: 1000 cost: 256.179565\n",
      "Iteration: 1200 cost: 250.946381\n",
      "Iteration: 1400 cost: 202.875122\n",
      "Iteration: 1600 cost: 263.229431\n",
      "Iteration: 1800 cost: 176.801727\n",
      "Iteration: 2000 cost: 180.986023\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(2001):\n",
    "        x, _= mnist.train.next_batch(batch_size)\n",
    "\n",
    "        _, c = sess.run([optimizer1, cost1], feed_dict={x_in: x, drop: 0.9})\n",
    "        if i % 200 == 0:\n",
    "            print(\"Iteration: %d cost: %f\" %(i, c))\n",
    "         \n",
    "    enc_layer_1 = enc_1_hidden.eval(feed_dict={x_in:mnist.train.images, drop: 1.0})\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2>Encoding Layer 2 Training</h2>\n",
    "\n",
    "We train the second encoding layer on the trained output of the first encoding layer (without noise). \n",
    "We then add noise to the second layer and attempt to recreate its input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch(x, y, batch_size):\n",
    "    index = list(range(len(x)))\n",
    "    batch_index = np.random.choice(index, batch_size)\n",
    "    return x[batch_index], y[batch_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with train_graph.as_default():\n",
    "    with tf.variable_scope('enc_tw'):\n",
    "        enc_w2 = tf.get_variable(\"enc_21\", shape=[n_hidden1, n_hidden2], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        enc_b2 = tf.get_variable('b_21', shape=(n_hidden2), initializer = tf.constant_initializer(0.1))\n",
    "        dec_greed_w2 = tf.get_variable('dec_greed_w21', shape=[n_hidden2, n_hidden1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        dec_greed_b2 = tf.get_variable('dec_greed_b21', shape=(n_hidden1), initializer=tf.constant_initializer(0.1))  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with train_graph.as_default():\n",
    "    enc_layer1 = tf.placeholder(tf.float32, (None, n_hidden1))\n",
    "    enc_2_hidden= tf.nn.elu(tf.add(tf.matmul( enc_layer1 + scale * tf.random_normal((n_hidden1,)), enc_w2), enc_b2))\n",
    "    dec_2_output = tf.nn.elu(tf.add(tf.matmul(enc_2_hidden, dec_greed_w2), dec_greed_b2))\n",
    "    cost2 = 0.5 * tf.reduce_sum(tf.pow(tf.subtract(enc_layer1, dec_2_output), 2.0))\n",
    "    with tf.variable_scope('optim51'):\n",
    "        optimizer2 = tf.train.AdamOptimizer(name='optim51').minimize(cost2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Setup to Use Encoder for Classification</h2>\n",
    "Now we will transform the inputs using the final layer of the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 cost: 68411.554688\n",
      "Iteration: 2000 cost: 4130.927734\n",
      "Iteration: 4000 cost: 3347.262207\n",
      "Iteration: 6000 cost: 2897.959961\n",
      "Iteration: 8000 cost: 3029.081299\n",
      "Iteration: 10000 cost: 3110.814453\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(10001):\n",
    "        x, _= batch(enc_layer_1 ,mnist.train.labels, batch_size)\n",
    "\n",
    "        _, c = sess.run([optimizer2, cost2], feed_dict={enc_layer1: x, drop: 0.9})\n",
    "        if i % 2000 == 0:\n",
    "            print(\"Iteration: %d cost: %f\" %(i, c))\n",
    "    \n",
    "    enc_layer_2 = enc_2_hidden.eval(feed_dict={enc_layer1:enc_layer_1, drop: 1.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "<h2>Classification Step Encoded Data</h2>\n",
    "Now that the model is trained, its just a matter of plugging the encoded data (50 dimensions) into a simple feed forward network to classify digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_graph = tf.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with class_graph.as_default():\n",
    "    y_ = tf.placeholder(tf.float32, (None, 10))\n",
    "    x = tf.placeholder(tf.float32, (None, 50)) \n",
    "    W = tf.Variable(tf.zeros([50, 10]))\n",
    "    b = tf.Variable(tf.zeros([10]))\n",
    "    y = tf.matmul(x, W) + b\n",
    "    \n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "    with tf.variable_scope('opti'):\n",
    "        train = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "    correct_ = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_, tf.float32))\n",
    "    \n",
    "#basic classifier derived from: \n",
    "#https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/examples/tutorials/mnist/mnist_softmax.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0 accuracy: 0.234375 \n",
      "Iteration:  500 accuracy: 0.882812 \n",
      "Iteration:  1000 accuracy: 0.890625 \n",
      "Iteration:  1500 accuracy: 0.968750 \n",
      "Iteration:  2000 accuracy: 0.914062 \n",
      "Iteration:  2500 accuracy: 0.859375 \n",
      "Iteration:  3000 accuracy: 0.914062 \n",
      "Iteration:  3500 accuracy: 0.906250 \n",
      "Iteration:  4000 accuracy: 0.921875 \n",
      "Iteration:  4500 accuracy: 0.906250 \n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=class_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer()) \n",
    "    for i in range(5000):\n",
    "        x2, y2 = batch(enc_layer_2, mnist.train.labels, batch_size)\n",
    "        sess.run(train, feed_dict={x:x2, y_:y2})\n",
    "        if i % 500 == 0:\n",
    "            print(\"Iteration:  %d accuracy: %f \" %(i, sess.run(accuracy, feed_dict={x: x2, y_:y2})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Classification Step Original Data</h2>\n",
    "We will now run the original data (784 dimensions) through the same network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_graph = tf.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with class_graph.as_default():\n",
    "    y_ = tf.placeholder(tf.float32, (None, 10))\n",
    "    x = tf.placeholder(tf.float32, (None, 784)) \n",
    "    W = tf.Variable(tf.zeros([784, 10]))\n",
    "    b = tf.Variable(tf.zeros([10]))\n",
    "    y = tf.matmul(x, W) + b\n",
    "    \n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "    with tf.variable_scope('opti'):\n",
    "        train = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "    correct_ = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_, tf.float32))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0 accuracy: 0.328125 \n",
      "Iteration:  500 accuracy: 0.960938 \n",
      "Iteration:  1000 accuracy: 0.898438 \n",
      "Iteration:  1500 accuracy: 0.937500 \n",
      "Iteration:  2000 accuracy: 0.960938 \n",
      "Iteration:  2500 accuracy: 0.921875 \n",
      "Iteration:  3000 accuracy: 0.914062 \n",
      "Iteration:  3500 accuracy: 0.929688 \n",
      "Iteration:  4000 accuracy: 0.929688 \n",
      "Iteration:  4500 accuracy: 0.929688 \n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=class_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer()) \n",
    "    for i in range(5000):\n",
    "        x2, y2 = batch(mnist.train.images, mnist.train.labels, batch_size)\n",
    "        sess.run(train, feed_dict={x:x2, y_:y2})\n",
    "        if i % 500 == 0:\n",
    "            print(\"Iteration:  %d accuracy: %f \" %(i, sess.run(accuracy, feed_dict={x: x2, y_:y2})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Conclusion</h2>\n",
    "\n",
    "By feeding the classifier an encoded version of the data we were able to achieve approx. 92% accuracy on the training set vs 96% on the original data. We can thus conclude that the encoded data that is roughly 16 times smaller than the original dataset has learned important features about the dataset. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
